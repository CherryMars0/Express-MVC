---
title: 高等数学与经济学初步
date: 2022-06-24 13:27:26
categories:
- math
- 经济学
tags: 
- 高等数学
- 经济学
- 导数与微积分
---

### 一.线性回归与商品规划

<img src="https://raw.githubusercontent.com/CherryMars0/blog-img/main/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png" />

线性回归是在金融数据分析中很基础的机器学习算法，本文将通俗易懂的介绍线性回归的基本概念，优缺点和逻辑回归的比较。

#### 几个重要的概念：

##### 	两种数据类型：

​	1.自变量：可以直接控制的数据。

​	2.因变量：不可以直接控制的数据。即需要预测和估算。

##### 	模型：

​	模型本质上就是一个转换引擎，主要的作用就是找到自变量和因变量之间的关系函数。

##### 	参数：

​	参数是添加到模型中用于输出预测的要素。

#### 什么是线性回归：

##### 	什么是回归：

回归是一种基于独立预测变量对目标值进行建模的方法。回归的目的主要是用于预测和找出变量之间的因果关系。比如预测明天的天气温度，预测股票的走势。回归之所以能预测是因为它通过历史数据，摸透了"套路"，然后通过这个套路来预测未来的结果。回归技术主要根据自变量的数量以及自变量和因变量之间的关系类型而有所不同。

##### 	什么是线性：

数据点排成一条直线（或接近直线），或者沿直线延长。线性意味着，因变量和自变量之间的关系可以用直线表示。例如：

​	「房子」越大，「租金」就越高

​	「金子」买的越多，花的「钱」就越多

​	杯子里的「水」越多，「重量」就越大

线性关系不仅仅只能存在 2 个变量（二维平面）。3 个变量时（三维空间），线性关系就是一个平面，4 个变量时（四维空间），线性关系就是一个体。

##### 	什么是线性回归：

线性回归本来是是统计学里的概念，现在经常被用在机器学习中。

线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。

回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

如果 2 个或者多个变量之间存在"线性关系"，那么我们就可以通过历史数据，摸清变量之间的"套路"，建立一个有效的模型，来预测未来的变量结果。

##### 	线性回归优特点：

​	特点：速度快，可解释性好，不适合非线性。

##### 	线性回归优点：

​	1.建模速度快，不需要很复杂的数据，在数据量大的情况下依然运行速度很快。

​	2.可以根据系数给出每个变量的解释和理解。

##### 	线性回归缺点：

​	1.不能很好地拟合非线性数据。所以需要先判断变量之间是否是线性关系。

##### 	注：

1.线性回归只能用于回归问题，逻辑回归虽然名字叫回归，但是更多用于分类问题。

2.线性回归要求因变量是连续性数值变量，而逻辑回归要求因变量是离散的变量。

3.线性回归要求自变量和因变量呈线性关系，而逻辑回归不要求自变量和因变量呈线性关系。

4.线性回归可以直观的表达自变量和因变量之间的关系，逻辑回归则无法表达变量之间的关系。

#### 一元线性回归应用：

线性回归的前提，是要有足够数据的支撑，虽有一些算法能弥补部分样本数据不够时所带来的问题。

最简单的线性回归每年都会出现在高考试卷当中：

例：假设有一个超市过去七天内，每天的平均温度和当天所卖出的冰激凌数量如下表，试求每天温度关于卖出冰激凌的线性回归方程。

|      X(温度）       |  25  |  26  |  27  |  28  |  29  |
| :-----------------: | :--: | :--: | :--: | :--: | :--: |
| Y（卖出冰激凌数量） |  10  |  15  |  17  |  23  |  25  |

$$
\bf \bar{x} =\frac {\sum_{i=1}^n {x_i}}{n}（线性回归方程-1）
$$

$$
\bf \bar{y} =\frac {\sum_{i=1}^n {y_i}}{n}（线性回归方程-2）
$$

$$
\bf \hat{b}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{(x_1-\bar{x})(y_1-\bar{y})+(x_2-\bar{x})(y_2-\bar{y})+(x_3-\bar{x})(y_3-\bar{y})+...}{(x_1-\bar{x})^2+(x_2-\bar{x})^2+...} （线性回归方程-3）
$$

$$
\bf \bar{y} = \hat{b}\bar{x} + \hat{a}（线性回归方程-4）
$$

$$
\bf L(y|f(x)) = \frac{1}{n} \sum_{i=1}^n(y_i-f(x_i))^2 （线性回归方程 - 5 - MSE(均方误差损失函数)）
$$

$$
\bf L(y|f(x)) = \sqrt[2]{\frac{1}{n} \sum_{i=1}^n(y_i-f(x_i))^2} （线性回归方程 - 6 - L2(欧氏误差函数)）
$$

$$
\bf L(y|f(x)) = \sum_{i=1}^n|y_i-f(x_i)|（线性回归方程 - 7 - L1(曼哈顿误差函数)）
$$

$$
\bf L(y|f(x)) = \begin{cases}  
 \frac{1}{2}(y-f(x))^2 & |y-f(x)| < 1 \\
 |y-f(x)|-\frac{1}{2} & |y-f(x)| \geq 1
\end{cases}（线性回归方程 - 8 - Smooth L1(Girshick损失函数)）
$$

$$
\bf L(y|f(x)) = \begin{cases}  
 \frac{1}{2}(y-f(x))^2 & |y-f(x)| \leq \delta \\
 \delta|y-f(x)|-\frac{1}{2}\delta^2 & |y-f(x)| > \delta
\end{cases}（线性回归方程 - 9 - huber(huber损失函数)）
$$

$$
\bf L(y|f(x)) = \sum_{i=1}^ny_i\times\log(\frac{y_i}{f(x_i)})（线性回归方程 - 10 - KL(散度函数)）
$$

$$
\bf L(y|f(x)) = - \sum_{i=1}^n y_i\log(f(x_i)) （线性回归方程 - 11 - ReLU(交叉熵损失函数)）
$$

$$
\bf L(y|f(x)) = -\frac{1}{n}\sum_{i=1}^n\log(\frac{e^{fy_i}}{\sum_{j=1}^ce{f_j}})（线性回归方程 - 12 - softmax(CNN模型损失函数)）
$$

`第一步`：先求出X，Y的平均值：
$$
\bar{x} =\frac {25+26+27+28+29}{5}=27（线性回归方程-1）
$$

$$
\bar{y} =\frac {10+15+17+23+25}{5}=18（线性回归方程-2）
$$

`第二步`：求线性回归方程的斜率：
$$
\hat{b}=\frac{(25-27)(10-18)+(26-27)(15-18)+(27-27)(17-18)+(28-27)(23-18)+(29-27)(25-18)}{(25-27)^2+(15-27)^2+(27-27)^2+(28-27)^2+(29-27)^2} = 3.8（线性回归方程-3）
$$
`第三步`：求线性回归方程的截距：
$$
\hat{a} = 18-3.8\times27=-84.6（线性回归方程-4）
$$
`第四步`：求线性回归方程：
$$
y = 3.8x - 84.6（线性回归方程-4）
$$
<img src="https://raw.githubusercontent.com/CherryMars0/blog-img/main/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E2%80%94%E4%BE%8B1.png" />

##### 神经网络在线性回归中的应用：

###### 		线性回归的程序实现：

``` python
import numpy as np
import matplotlib.pyplot as plt

x = [25, 26, 27, 28, 29] #温度
y = [10, 15, 17, 23, 25] #卖出的数量

def linearRegression(x: list, y: list):
    def Average(x: list, y: list): #求平均
        return (sum(x)/len(x), sum(y)/len(y))
    x_bar, y_bar = Average(x, y)

    def slope(x: list, y: list, x_bar, y_bar): #求斜率
        j = 0
        k = 0
        for x, y in zip(x, y):
            j += (x-x_bar)*(y-y_bar)
            k += (x-x_bar)*(x-x_bar)
        return(j/k)
    b_hat = slope(x, y, x_bar, y_bar)

    def intercept(x_bar, y_bar, b_hat): #求截距
        return (y_bar-b_hat*x_bar)
    a_hat = intercept(x_bar, y_bar, b_hat)
    print("x_bar:", x_bar, "y_bar:", y_bar)
    print("a_hat:", a_hat, "b_hat:", b_hat)

    def showRegression(a_hat, b_hat, x: list, y: list): #画图
        plt.figure(figsize=(10, 10), dpi=100)
        X = np.linspace(max(x), min(x), 1000)
        Y = b_hat * X + a_hat
        plt.plot(X, Y, color='red', linestyle="--",
                label=("y={}x{}".format(b_hat, a_hat)))
        plt.scatter(x, y, color='black', label="real")
        plt.legend(loc="best")
        plt.show()
    showRegression(a_hat, b_hat, x, y)
    
linearRegression(x, y)

执行结果：
x_bar: 27.0   y_bar: 18.0
a_hat: -84.6  b_hat: 3.8
MSEloss: 0.72
```

### 二.导数与商品边际效应

#### 几个重要概念：

`边际利润`:边际利润是指每增加一单位产品所增加的利润。

​	现有一单成本函数：即生产 x 单位产品需要 y 的成本，则其函数的边际利润就是其单成本函数的导数。

`边际成本`:边际成本是指每增加一单位产品所增加的总成本。

​	现有一总成本导数：即生产 x 单位产品需要 y 的总成本，则其函数的边际成本就是其总成本函数的导数。

`边际收入`:边际收入指是每增加一单位产品所增加的收入额。

​	现有一总收入导数：即生产 x 单位产品会产出 y 的总收入，则其函数的边际收入就是其总收入函数的导数。

`利润最大化`：边际收益等于边际成本时利润最大化。

有了上述概念以后，我们就可以根据每个概念中的自变量和应变量收集数据然后来拟合出三种回归函数（单成本函数，总成本函数，总收入函数），进而定量的分析整个商品的走向。

在第一节中，简单的介绍了线性回归和一个关于线性回归最简单的实际问题，而实际问题往往并没有那么简单，如例2：



### 三.回归函数与商品定价
